# Benchmarks: PyTorchÂ â†’Â StableHLOÂ â†’Â IREE

A **reproducible microâ€‘benchmark suite** for testing how common PyTorch models compile and run across multiple backâ€‘ends (CPU, CUDA, Vulkan, ROCm) using the OpenXLAÂ /Â IREE toolchain.

---

## ğŸ“ Repository layout

```text
benchmarks/
â”œâ”€â”€ env/                      # Preâ€‘baked Conda env specs (pick one)
â”‚Â Â  â”œâ”€â”€ iree-cpu/             # IREE CPU
â”‚Â Â  â”œâ”€â”€ torch-mlir/           # PyTorch to StableHLO through torch-mlir (for both CPU and CUDA)
â”‚Â Â  â”œâ”€â”€ torch-xla/            # PyTorch to StableHLO through torch-xla (for CPU)
â”‚Â Â  â””â”€â”€ stablehlo-iree/       # StableHLO to IREE
â”œâ”€â”€ models/                   # Model wrappers - get_model(), get_dummy_input()
â”‚Â Â  â”œâ”€â”€ conv_block.py
â”‚Â Â  â”œâ”€â”€ mobilenet_block.py
â”‚Â Â  â”œâ”€â”€ bert_block.py
â”‚Â Â  â”œâ”€â”€ vit_block.py
â”‚Â Â  â”œâ”€â”€ gpt2_block.py
â”‚Â Â  â”œâ”€â”€ llama_block.py
â”‚Â Â  â”œâ”€â”€ deepseek_block.py
â”‚Â Â  â”œâ”€â”€ gcn_block.py
â”‚Â Â  â”œâ”€â”€ graphsage_block.py
â”‚Â Â  â”œâ”€â”€ gat_block.py
â”‚Â Â  â””â”€â”€ gatv2_block.py
â”œâ”€â”€ scripts/                        # All entryâ€‘point helpers
â”‚Â Â  â”œâ”€â”€ run_bench.py                # Pureâ€‘PyTorch latency probe
â”‚Â Â  â”œâ”€â”€ run_bench_memgraph.py       # Memory Graph
â”‚Â Â  â”œâ”€â”€ export_torch_mlir_stablehlo # export StableHLO through torch-mlir 
â”‚Â Â  â”œâ”€â”€ export_torch_xla_stablehlo  # export StableHLO through torch-xla 
â”‚Â Â  â”œâ”€â”€ compile_run_xla.py          # PyTorch -> StableHLO -> XLA
â”‚Â Â  â”œâ”€â”€ compile_run_iree.py         # PyTorch -> Linalg -> IREE
â”‚Â Â  â””â”€â”€ ireeflow
â”‚Â Â   Â Â  â”œâ”€â”€ save_mlir_and_params.py          # Save Torch Dialect MLIR and Parameters
â”‚Â Â   Â Â  â”œâ”€â”€ compile_vmfb.py                  # Compile to VMFB
â”‚Â Â   Â Â  â”œâ”€â”€ run_model.py                     # Run - TODO: Accuracy Comparison
â”‚Â Â   Â Â  â”œâ”€â”€ measure_run.py                   # Latency Measurement
â”‚Â Â   Â Â  â”œâ”€â”€ cuda_compile_vmfb.py             # Compile to VMFB (CUDA Ver.)
â”‚Â Â   Â Â  â”œâ”€â”€ cuda_run_model.py                # Run (CUDA Ver.) - TODO: Accuracy Comparison
â”‚Â Â   Â Â  â””â”€â”€ cuda_measure_run.py              # Latency Measurement (CUDA Ver.)
â””â”€â”€ results/                        # *.vmfb, *.csv outputs are dropped here
```

---

## ğŸš€ QuickÂ start

### 1Â Â·Â Clone & create env (PythonÂ 3.11)

```bash
# clone your private repo
git clone git@github.com:HaeeunJeong/benchmarks.git && cd benchmarks

conda env create --file ./env/{target_env}/environment.yaml
conda activate {target_env}
```



### 2Â Â·Â Run the plain PyTorch microâ€‘benchmarks

```bash
# Usage
python -m scripts.run_bench {model} --device {cpu/cuda} --csv

# Ex. run all models on CPU
python -m scripts.run_bench --device cpu --csv

# Ex. single model on GPU
python -m scripts.run_bench resnet --device cuda --csv
```

```bash
# Convert PyTorch module into stablehlo through torch-xla
$ python -m scripts.torch_xla_stablehlo

# Convert PyTorch module into stablehlo through torch-mlir
$ python -m scripts.torch_mlir_stablehlo
```

*Outputs*: perâ€‘model latency is printed and appended to `results/latency.csv` (timestamp,Â model,Â device,Â timeÂ /Â "error").

---

## ğŸ—ï¸ Supported models

| Category        | Key                 | Source                                                | Notes                                                                         |
| --------------- | ------------------- | ----------------------------------------------------- | ----------------------------------------------------------------------------- |
| SimpleÂ custom   | `conv`              | `models/conv_block.py`                                | 2Ã—ConvÂ +Â ReLU toy block used as a sanityâ€‘check kernel                         |
| **GNN**         | `gcn`\*             | `models/gcn_block.py`                                 | Graph Convolution Net                                                         |
| **GNN**         | `graphsage`\*       | PyTorchÂ Geometric                                     | GraphÂ Sage Net; The milestone of GNN model                                    |
| **GNN**         | `gat`\*             | PyTorchÂ Geometric                                     | GraphÂ AttentionÂ Net; contains `scatter_reduce`/`nonzero` (exportâ€‘unsupported) |
| **GNN**         | `gatv2`\*           | PyTorchÂ Geometric                                     | GraphÂ AttentionÂ Net; contains `scatter_reduce`/`nonzero` (exportâ€‘unsupported) |
| **CNN**         | `resnet`            | `torchvision`Â ResNetâ€‘18                               | Classic image backbone; ImageNet pretrained                                   |
| **CNN**         | `mobilenet`         | `torchvision`Â MobileNetÂ v3Â S                          | Mobileâ€‘oriented CNN; ImageNet pretrained                                      |
| **Transformer** | `vit`               | `torchvision`Â ViTâ€‘B/16                                | Vision Transformer baseline                                                   |
| **Transformer** | `bert`\*            | `bert-base-uncased`                                   | Tokenâ€‘level encoder; needs full kwargs to export                              |
| **Transformer** | `gpt2`\*            | `gpt2-xl`                                             | 1.5â€¯Bâ€‘param decoder; export kwargs WIP                                        |
| **LLM**         | `llama`\*           | `meta-llama/Llama-3.2-3B`                             | Base LlamaÂ 3.2Â 3â€¯B; compact generalâ€‘purpose LLM                               |
| **LLM**         | `llama-qlora`\*     | `meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8`     | 4â€‘bit QLoRA; memoryâ€‘efficient inference                                       |
| **LLM**         | `llama-spinquant`\* | `meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8` | 4â€‘bitÂ SpinQuant; alt quantization scheme                                      |
| **LLM**         | `deepseek`\*        | `DeepSeekâ€‘R1â€‘Distillâ€‘Qwen`                            | Distilled mathâ€‘centric LLM; complex TF graph                                  |

<sup>\*Â asterisked entries are currently skipped in `compile_iree.py` (seeÂ `UNSUPPORTED`).</sup>
<sup>\*Â marked entries are currently skipped in compile\_iree.py (seeÂ `UNSUPPORTED`).</sup>

---

## ğŸ”§ Compiling & running with IREE

```bash
# compile all supported vision models to LLVMâ€‘CPU & store vmfb
$ python -m scripts.compile_iree --target llvm-cpu --csv

# pick a specific model & CUDA backâ€‘end (needs GPU + CUDA driver)
$ python -m scripts.compile_iree resnet mobilenet --target cuda
```

*Outputs*

* **FlatBuffer (`.vmfb`)**: saved to `results/<model>-<target>.vmfb`
* **Latency CSV**: appended to `results/iree_latency.csv` with status `timeÂ (ms)`Â /Â `error`Â /Â `unsupported`.

---

## â• Adding new models

1. Drop a new Python module under `models/`, e.g. `my_model.py`.
2. Expose `(model, dummy_input)` via a helper (see existing examples).
3. Add its key to `scripts/run_bench.py` â†’ `load_model()` and to `ALL_MODELS`.
4. If the model exports cleanly to StableHLO, remove it from `UNSUPPORTED` in `compile_iree.py`.

---

## ğŸ› ï¸ Troubleshooting

| Symptom                                           | Fix                                                                                                 |
| ------------------------------------------------- | --------------------------------------------------------------------------------------------------- |
| `UserWarning: 'pretrained' is deprecated`         | TorchVision â‰¥0.13 uses `weights=..._Weights.DEFAULT`; already handled.                              |
| `This model contains ops not capturable ...`      | Operation not yet supported by Torchâ€‘XLA; keep model in `UNSUPPORTED` or write a Torchâ€‘FX fallback. |
| `treespec.unflatten` during export                | Provide full kwargs (`attention_mask`, etc.) matching the modelâ€™s forward signature.                |
| `expected operation name in quotes` at IREE stage | Ensure you pass **the actual MLIR string**, not a Python repr (use `str(shlo.mlir_module)`).        |

---

## ğŸ“œ License / visibility

This is a **private** research benchmark. Do **NOT** publish benchmark numbers externally without permission.
